{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import mplfinance as fplt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Ticker Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in historical data from Yahoo Finance for a specified ticker\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def get_ticker_data(ticker, period, interval):\n",
    "    '''This function pulls historical data for a specified ticker from Yahoo Finance using yfinance.\n",
    "    If you do not have yfinance installed, run pip install yfinance\n",
    "    If the current trading day has not closed it will be removed from the data.\n",
    "    \n",
    "    Inputs:\n",
    "        ticker = ticker symbol for the trade instrument. (str)\n",
    "        period = data period to download \n",
    "            available options: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "        interval = data interval.\n",
    "            available options 1d, 5d, 1wk, 1mo, 3mo     \n",
    "    '''\n",
    "    # set folder and file variables\n",
    "    folder_name = 'Data'\n",
    "    file_name = ticker\n",
    "    file = os.path.join(folder_name, file_name + '.csv')\n",
    "    p = Path(folder_name)\n",
    "    \n",
    "    # Get data amd drop the current trading day\n",
    "    data = yf.Ticker(ticker)\n",
    "    data_df = data.history(period=period,\n",
    "                                interval=interval,\n",
    "                                actions=False,\n",
    "                                back_adjust=True).reset_index()\n",
    "    data_df = data_df.iloc[:-1]\n",
    "    \n",
    "    # Check if folder and file exist\n",
    "    if os.path.exists(file):\n",
    "        \n",
    "    # Read in historical data CSV, convert dtype of Date column, append DFs, drop dups, and overwrite csv  \n",
    "        historical_data = pd.read_csv(file)\n",
    "        historical_data['Date'] = pd.to_datetime(historical_data.Date)\n",
    "        historical_data = (historical_data.append(data_df)\n",
    "                  .reset_index()\n",
    "                  .drop(columns='index')\n",
    "                 )\n",
    "        data_df = historical_data.drop_duplicates(keep='last')\n",
    "        data_df.to_csv(Path(file), index=False) \n",
    "        strSuccess = f'Appended data to {file}.'\n",
    "        \n",
    "    # Check if directory exist and save df to Data folder as a csv using the ticker as the name\n",
    "    elif os.path.isdir(folder_name):\n",
    "        data_df.to_csv(Path(p, ticker + '.csv'), index=False)\n",
    "        strSuccess = f'Wrote your file to the {p} folder as {file_name}.csv.'\n",
    "        \n",
    "    # make the Data folder and save df to csv using the ticker as the name\n",
    "    else:\n",
    "        os.makedirs(folder_name)\n",
    "        data_df.to_csv(Path(p, ticker + '.csv'), index=False)\n",
    "        strSuccess = f'A folder named, {folder_name}, was created and you file was save in it as {file_name}.csv.'\n",
    "    return(strSuccess, data_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites used to get twitter accounts to look at\n",
    "# https://www.offshore-technology.com/features/top-influencers-in-oil-and-gas/\n",
    "# https://www.benzinga.com/markets/commodities/15/02/5252239/10-oil-experts-to-follow-on-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "from tweepy import OAuthHandler\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def get_twitter_data(screen_name):\n",
    "    # load data for auth to twitter\n",
    "    load_dotenv(dotenv_path=\"C:/Users/brett/Desktop/FTBC/.env\")\n",
    "    api_key = os.getenv('TWITTER_API_KEY')\n",
    "    api_secret_key = os.getenv('TWITTER_SECRET_KEY')\n",
    "    bearer_token = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "    access_token = os.getenv('TWITTER_ACCESS_TOKEN')\n",
    "    access_s_token = os.getenv(\"TWITTER_SECRET_TOKEN\")\n",
    "    \n",
    "    auth = tw.OAuthHandler(api_key, api_secret_key)\n",
    "    auth.set_access_token(access_token, access_s_token)\n",
    "\n",
    "    api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    # First request to get max count allowed to initialize user_tweets varible to allow iterating to get more tweets\n",
    "    tweets = []\n",
    "    total_tweets = 0\n",
    "    user_tweets = api.user_timeline(screen_name = screen_name,\n",
    "                                    include_rts=False,\n",
    "                                    tweet_mode='extended',\n",
    "                                    count = 200)\n",
    "    # add user_tweets to tweets list\n",
    "    tweets.extend(user_tweets)\n",
    "    total_tweets += len(user_tweets)\n",
    "\n",
    "    # loop to pull max amount of tweets allowed by twitter (3200)\n",
    "    while len(user_tweets) > 0:\n",
    "        # varialbe to be used as max_id for loop\n",
    "        last_id = user_tweets[-1].id - 1\n",
    "        \n",
    "        user_tweets = api.user_timeline(screen_name = screen_name,\n",
    "                                        max_id = last_id - 1,\n",
    "                                        include_rts=False,\n",
    "                                        tweet_mode='extended',\n",
    "                                        count = 200)\n",
    "        \n",
    "        # add additional tweets to original list\n",
    "        tweets.extend(user_tweets)\n",
    "        \n",
    "        print(f'{len(user_tweets)} have been downloaded for {screen_name}')\n",
    "        total_tweets += len(user_tweets) \n",
    "    \n",
    "    print(f'{total_tweets} downloaded for {screen_name}')\n",
    "    # extract the data that is needed from the tweet data\n",
    "    tweet_details = [[tweet.user.screen_name, tweet.created_at, tweet.full_text, tweet.favorite_count, tweet.retweet_count] for tweet in tweets]\n",
    "    \n",
    "    # Push extracted data to csv for use later\n",
    "    with open(f'Data/{screen_name}_tweets.csv', 'w', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['screen_name', 'date', 'tweet', 'likes', 'retweets'])\n",
    "        writer.writerows(tweet_details)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to view the attributes of the tweet data\n",
    "# import pprint\n",
    "# status = user_tweets[0]\n",
    "# # convert to string\n",
    "# json_str = json.dumps(status._json)\n",
    "# # deserialise string into python object\n",
    "# parsed = json.loads(json_str)\n",
    "# print(json.dumps(parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting 2 Y Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_twoYs (df, colName0, colName1, colName2):\n",
    "    #create two axis\n",
    "    # create figure and axis objects with subplots()\n",
    "    fig,ax = plt.subplots(figsize=(30,10))\n",
    "    # make a plot\n",
    "    ax.plot(df.index, df[colName1], color=\"red\",\n",
    "#             marker=\"o\",\n",
    "           )\n",
    "    # set x-axis label\n",
    "    ax.set_xlabel('Date',fontsize=14)\n",
    "    # set y-axis label\n",
    "    ax.set_ylabel(colName1,color=\"red\",fontsize=14)\n",
    "    \n",
    "    # twin object for two different y-axis on the sample plot\n",
    "    ax2=ax.twinx()\n",
    "    # make a plot with different y-axis using second axis object\n",
    "    ax2.plot(df.index, df[colName2],color=\"blue\",\n",
    "#              marker=\"o\",\n",
    "            )\n",
    "    ax2.set_ylabel(colName2,color=\"blue\",fontsize=14)\n",
    "    plt.show()\n",
    "    # save the plot as a file\n",
    "#     fig.savefig(fileName + '.jpg',\n",
    "#                 format='jpeg',\n",
    "#                 dpi=100,\n",
    "#                 bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data from Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "cl_path = Path('Data/CL=F.csv')\n",
    "uup_path = Path('Data/UUP.csv')\n",
    "\n",
    "# Read files to a dataframe\n",
    "cl_df = pd.read_csv(cl_path, index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "uup_df = pd.read_csv(uup_path, index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "\n",
    "# Drop columns to have just closing price\n",
    "cl_close = cl_df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n",
    "uup_close = uup_df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n",
    "\n",
    "# Change column name to ticker name\n",
    "cl_close.columns = ['CL']\n",
    "uup_close.columns = ['UUP']\n",
    "\n",
    "# # Combine the two dataframes\n",
    "combined_cl_uup = pd.concat([cl_close, uup_close], axis='columns', join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>tweets_no_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>Tune in now for a livestream of the dedication of T. Boone Pickens’ childhood home, the Holdenville House, at his final resting place at Karsten Creek Golf Club in Stillwater, OK. #okstate #Rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>Tomorrow is the one-year anniversary of the passing of the legendary T. Boone Pickens. We invite you to a livestream at 12:32 pm CT for the official opening of his childhood home, the Holdenville ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>Grateful for the impactful summary of Boone’s final letter @TheRetirementManifesto #spreadtheword – Jay Rosser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>Thank you for this meaningful nod to Boone. He was proud to be associated with such an impactful organization as @NFFNetwork The highlight at the end should bring a chuckle to everyone at #okstate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>.@FortuneMagazine revisits some of T. Boone Pickens' most memorable quotes throughout the years. #RememberingBoone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name       date  likes  retweets                                                                                                                                                                                            tweets_no_url\n",
       "0  boonepickens 2020-09-11     16         3  Tune in now for a livestream of the dedication of T. Boone Pickens’ childhood home, the Holdenville House, at his final resting place at Karsten Creek Golf Club in Stillwater, OK. #okstate #Rememb...\n",
       "1  boonepickens 2020-09-10    120        24  Tomorrow is the one-year anniversary of the passing of the legendary T. Boone Pickens. We invite you to a livestream at 12:32 pm CT for the official opening of his childhood home, the Holdenville ...\n",
       "2  boonepickens 2020-01-09     37        10                                                                                          Grateful for the impactful summary of Boone’s final letter @TheRetirementManifesto #spreadtheword – Jay Rosser \n",
       "3  boonepickens 2020-01-08     96        12  Thank you for this meaningful nod to Boone. He was proud to be associated with such an impactful organization as @NFFNetwork The highlight at the end should bring a chuckle to everyone at #okstate...\n",
       "4  boonepickens 2019-10-18     53         5                                                                                      .@FortuneMagazine revisits some of T. Boone Pickens' most memorable quotes throughout the years. #RememberingBoone "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in json files\n",
    "boonepickens_path = Path('Data/boonepickens_tweets.csv')\n",
    "chrismartenson_path = Path('Data/chrismartenson_tweets.csv')\n",
    "chrisnelder_path = Path('Data/chrisnelder_tweets.csv')\n",
    "collineatonhc_path = Path('Data/CollinEatonHC_tweets.csv')\n",
    "gasbuddyguy_path = Path('Data/GasBuddyGuy_tweets.csv')\n",
    "jendlouhyhc_path = Path('Data/jendlouhyhc_tweets.csv')\n",
    "jkempenergy_path = Path('Data/JKempEnergy_tweets.csv')\n",
    "robinenergy_path = Path('Data/robinenergy_tweets.csv')\n",
    "staunovo_path = Path('Data/staunovo_tweets.csv')\n",
    "thearorareport_path = Path('Data/TheAroraReport_tweets.csv')\n",
    "\n",
    "# Convert to dfs\n",
    "boonepickens_df = pd.read_csv(boonepickens_path, parse_dates=True, infer_datetime_format=True)\n",
    "chrismartenson_df = pd.read_csv(chrismartenson_path, parse_dates=True, infer_datetime_format=True)\n",
    "chrisnelder_df = pd.read_csv(chrisnelder_path, parse_dates=True, infer_datetime_format=True)\n",
    "collineatonhc_df = pd.read_csv(collineatonhc_path, parse_dates=True, infer_datetime_format=True)\n",
    "gasbuddyguy_df = pd.read_csv(gasbuddyguy_path, parse_dates=True, infer_datetime_format=True)\n",
    "jendlouhyhc_df = pd.read_csv(jendlouhyhc_path, parse_dates=True, infer_datetime_format=True)\n",
    "jkempenergy_df = pd.read_csv(jkempenergy_path, parse_dates=True, infer_datetime_format=True)\n",
    "robinenergy_df = pd.read_csv(robinenergy_path, parse_dates=True, infer_datetime_format=True)\n",
    "staunovo_df = pd.read_csv(staunovo_path, parse_dates=True, infer_datetime_format=True)\n",
    "thearorareport_df = pd.read_csv(thearorareport_path, parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Combine into 1 dataframe\n",
    "all_tweets_df = pd.concat([boonepickens_df, chrismartenson_df,\n",
    "                         chrisnelder_df, collineatonhc_df,\n",
    "                         gasbuddyguy_df, jendlouhyhc_df,\n",
    "                         jkempenergy_df, robinenergy_df,\n",
    "                         staunovo_df, thearorareport_df]).reset_index(drop=True)\n",
    "\n",
    "# Normalize the Date field\n",
    "all_tweets_df['date'] = pd.to_datetime(all_tweets_df['date'], errors='coerce')\n",
    "all_tweets_df['date'] = all_tweets_df['date'].dt.normalize()\n",
    "\n",
    "# Remove the url from the tweet\n",
    "tweets_wo_url = []\n",
    "for tweet in all_tweets_df['tweet']:\n",
    "    no_url = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweets_wo_url.append({\n",
    "            \"tweets_no_url\": no_url}\n",
    "    )\n",
    "tweets_wo_url_df = pd.DataFrame(tweets_wo_url)\n",
    "\n",
    "# Append no_url_tweets to all_tweets_df and remove tweets column\n",
    "all_tweets_df= (pd.merge(all_tweets_df, tweets_wo_url_df, left_index=True, right_index=True, how='inner')\n",
    "                .drop(columns='tweet')\n",
    "               )\n",
    "\n",
    "# Display df\n",
    "pd.set_option('max_colwidth', 200)\n",
    "all_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Plots for Ticker Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df.plot(y='Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot_twoYs(combined_cl_uup, combined_cl_uup.index, 'CL', 'UUP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\brett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets_sentiment = []\n",
    "\n",
    "for tweet in all_tweets_df['tweets_no_url']:\n",
    "    try:\n",
    "        text = tweet\n",
    "        sentiment = analyzer.polarity_scores(tweet)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "\n",
    "        tweets_sentiment.append({\n",
    "            \"vader compound\": compound,\n",
    "            \"vader positive\": pos,\n",
    "            \"vader negative\": neg,\n",
    "            \"vader neutral\": neu \n",
    "        })\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# create Df\n",
    "vader_df = pd.DataFrame(tweets_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader compound</th>\n",
       "      <th>vader negative</th>\n",
       "      <th>vader neutral</th>\n",
       "      <th>vader positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vader compound  vader negative  vader neutral  vader positive\n",
       "0          0.4466             0.0          0.914           0.086\n",
       "1          0.5473             0.0          0.903           0.097\n",
       "2          0.4588             0.0          0.800           0.200\n",
       "3          0.9067             0.0          0.699           0.301\n",
       "4          0.0000             0.0          1.000           0.000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 100)\n",
    "vader_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textblob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textblob Polarity</th>\n",
       "      <th>Textblob Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Textblob Polarity  Textblob Subjectivity\n",
       "0           0.250000               0.750000\n",
       "1           0.533333               0.633333\n",
       "2           0.000000               1.000000\n",
       "3           0.433333               0.666667\n",
       "4           0.500000               0.750000"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports needed\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Analysis\n",
    "blob_sentiment = []\n",
    "\n",
    "for tweet in all_tweets_df['tweets_no_url']:\n",
    "    blob = TextBlob(tweet)\n",
    "    blob.sentiment\n",
    "    polarity = blob.sentiment[0]\n",
    "    subjectivity = blob.sentiment[1]\n",
    "\n",
    "    blob_sentiment.append({\n",
    "        'Textblob Polarity': polarity,\n",
    "        'Textblob Subjectivity': subjectivity\n",
    "    })\n",
    "\n",
    "# create Df\n",
    "textblob_df = pd.DataFrame(blob_sentiment)\n",
    "textblob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>tweets_no_url</th>\n",
       "      <th>vader compound</th>\n",
       "      <th>vader negative</th>\n",
       "      <th>vader neutral</th>\n",
       "      <th>vader positive</th>\n",
       "      <th>Textblob Polarity</th>\n",
       "      <th>Textblob Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>Tune in now for a livestream of the dedication of T. Boone Pickens’ childhood home, the Holdenvi...</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>Tomorrow is the one-year anniversary of the passing of the legendary T. Boone Pickens. We invite...</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>Grateful for the impactful summary of Boone’s final letter @TheRetirementManifesto #spreadthewor...</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>Thank you for this meaningful nod to Boone. He was proud to be associated with such an impactful...</td>\n",
       "      <td>0.9067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boonepickens</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>.@FortuneMagazine revisits some of T. Boone Pickens' most memorable quotes throughout the years....</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name       date  likes  retweets                                                                                        tweets_no_url  vader compound  vader negative  vader neutral  vader positive  Textblob Polarity  Textblob Subjectivity\n",
       "0  boonepickens 2020-09-11     16         3  Tune in now for a livestream of the dedication of T. Boone Pickens’ childhood home, the Holdenvi...          0.4466             0.0          0.914           0.086           0.250000               0.750000\n",
       "1  boonepickens 2020-09-10    120        24  Tomorrow is the one-year anniversary of the passing of the legendary T. Boone Pickens. We invite...          0.5473             0.0          0.903           0.097           0.533333               0.633333\n",
       "2  boonepickens 2020-01-09     37        10  Grateful for the impactful summary of Boone’s final letter @TheRetirementManifesto #spreadthewor...          0.4588             0.0          0.800           0.200           0.000000               1.000000\n",
       "3  boonepickens 2020-01-08     96        12  Thank you for this meaningful nod to Boone. He was proud to be associated with such an impactful...          0.9067             0.0          0.699           0.301           0.433333               0.666667\n",
       "4  boonepickens 2019-10-18     53         5  .@FortuneMagazine revisits some of T. Boone Pickens' most memorable quotes throughout the years....          0.0000             0.0          1.000           0.000           0.500000               0.750000"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all_tweets_df with the 2 sentiment dataframes\n",
    "tweet_sentiment_df = pd.concat([all_tweets_df, vader_df, textblob_df], axis=\"columns\", join='inner')\n",
    "pd.set_option('max_colwidth', 100)\n",
    "tweet_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Df to csv file\n",
    "tweet_sentiment_df.to_csv('Data/tweet_sentiment_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMB Watson Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports needed\n",
    "# from ibm_watson import ToneAnalyzerV3\n",
    "# from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "# from pandas.io.json import json_normalize \n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    " \n",
    "# # Get IBM tone keys\n",
    "# load_dotenv(dotenv_path=\"C:/Users/brett/Desktop/FTBC/.env\")\n",
    "# tone_api = os.getenv(\"IBM_TONE_API\")\n",
    "# tone_url = os.getenv(\"IBM_TONE_URL\")\n",
    "\n",
    "# # Create auth object\n",
    "# authenticator = IAMAuthenticator(tone_api)\n",
    "\n",
    "# # Create analyser instances\n",
    "# tone_analyzer = ToneAnalyzerV3(version=\"2017-09-21\", authenticator=authenticator)\n",
    "\n",
    "# # Set endpoint\n",
    "# tone_analyzer.set_service_url(tone_url)\n",
    "\n",
    "# # Analyze tone of tweets\n",
    "# for tweet in all_tweets_df['tweets_no_url']:\n",
    "#     tone_analysis = tone_analyzer.tone(tone_input,\n",
    "#                                        content_type=\"text/plan\",\n",
    "#                                        content_language=\"en\",\n",
    "#                                        accept_language=\"en\").get_result()\n",
    "\n",
    "# doc_tone_df = json_normalize(data=tone_analysis[\"document_tone\"], record_path=[\"tones\"])\n",
    "# doc_tone_df.head()\n",
    "\n",
    "# ibm_tone_df = json_normalize(\n",
    "#     data=tone_analysis[\"sentences_tone\"],\n",
    "#     record_path=[\"tones\"],\n",
    "#     meta=[\"sentence_id\", \"text\"],\n",
    "# )\n",
    "# sentences_tone_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp(tweets_str)\n",
    "\n",
    "# adj = [token.text for token in tokens if token.pos_ == 'ADJ']\n",
    "# print(adj)\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token.text, token.dep_)\n",
    "\n",
    "# displacy.render(tokens, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Plots of CandleStick charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Plotly Graph Objects\n",
    "fig = go.Figure(data=[go.Candlestick(x=cl_df.index,\n",
    "                open=cl_df['Open'],\n",
    "                high=cl_df['High'],\n",
    "                low=cl_df['Low'],\n",
    "                close=cl_df['Close'])])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With mplfinance from Matplotlib\n",
    "fplt.plot(cl_df.iloc[-45:],\n",
    "         type='candle',\n",
    "          style='yahoo',\n",
    "         title='CL Chart',\n",
    "         ylabel='Price',\n",
    "          mav=(9,20),\n",
    "          figratio=(12,5),\n",
    "          volume=True,\n",
    "#           savefig='Data/cl_last_45_days.png',\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Bokeh\n",
    "from math import pi\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import output_file\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.io import show\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "df = cl_df.iloc[-50:]\n",
    "# df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# # Needed of using the .rect to make the bars\n",
    "# mids = (df.Open + df.Close)/2\n",
    "# spans = df.Close-df.Open\n",
    "\n",
    "# Calculate 9 and 20 day SMA\n",
    "short_sma = cl_df['Close'].rolling(window=9).mean()\n",
    "long_sma = cl_df['Close'].rolling(window=40).mean()\n",
    "\n",
    "inc = df.Close > df.Open\n",
    "dec = df.Open > df.Close\n",
    "w = 12*60*60*1000\n",
    "\n",
    "# output_file(\"candlestick.html\", title=\"candlestick.py example\")\n",
    "# output_notebook(resources=INLINE)\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "p = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=800, plot_height=300, toolbar_location=\"left\", title='Last 50 Days of CL')\n",
    "\n",
    "# Creates the high and low wick\n",
    "p.segment(df.index, df.High, df.index, df.Low, color=\"black\")\n",
    "\n",
    "# Creates the body of the candlestick for the up days\n",
    "# p.rect(df.index[inc], mids[inc], w, spans[inc], fill_color=\"#D5E1DD\", line_color=\"black\")\n",
    "p.vbar(df.index[inc], w, df.Open[inc], df.Close[inc], fill_color=\"lawngreen\", line_color=\"black\")\n",
    "\n",
    "# Creates the body of the candlestick foor the down days\n",
    "# p.vbar(df.index[dec], mids[dec], w, spans[dec], fill_color=\"#F2583E\", line_color=\"black\")\n",
    "p.vbar(df.index[dec], w, df.Open[dec], df.Close[dec], fill_color=\"tomato\", line_color=\"black\")\n",
    "\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "\n",
    "# make the grid lines lighter\n",
    "p.grid.grid_line_alpha=0.3\n",
    "\n",
    "show(p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not going to need for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investing.com Data\n",
    "# file paths\n",
    "# spgsci_ER_path = Path('Data/spgsci_ER.csv')\n",
    "# spgsci_TR_path = Path('Data/spgsci_TR.csv')\n",
    "\n",
    "# Read files to a df\n",
    "# spgsci_ER_df = pd.read_csv(spgsci_ER_path, thousands=',', index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "# spgsci_TR_df = pd.read_csv(spgsci_TR_path, index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "\n",
    "# Change column name Price to Close\n",
    "# spgsci_ER_df = spgsci_ER_df.rename(columns={'Price': 'Close'})\n",
    "# spgsci_TR_df = spgsci_TR_df.rename(columns={'Price': 'Close'})\n",
    "\n",
    "# Drop Volume columns as it is blank\n",
    "# spgsci_ER_df = spgsci_ER_df.drop(columns='Vol.')\n",
    "# spgsci_TR_df = spgsci_TR_df.drop(columns='Vol.')\n",
    "\n",
    "# Drop %Change\n",
    "# spgsci_ER_df = spgsci_ER_df.drop(columns='Change %')\n",
    "# spgsci_TR_df = spgsci_TR_df.drop(columns='Change %')\n",
    "\n",
    "# # Add ticker symbol to df\n",
    "# spgsci_ER_df['Ticker'] = 'SPGSCLP'\n",
    "# spgsci_TR_df['Ticker'] = 'SPGSCL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two csv files for SP GSCI ER\n",
    "# # file paths\n",
    "# spgsci1_path = Path('Data/spgsci_ER_00_19.csv')\n",
    "# spgsci2_path = Path('Data/spgsci_ER_19_20.csv')\n",
    "\n",
    "# # Read files to a df\n",
    "# spgsci1_df = pd.read_csv(spgsci1_path, index_col='Date', parse_dates=True, infer_datetime_format=True, dtype=object).sort_index()        \n",
    "# spgsci2_df = pd.read_csv(spgsci2_path, index_col='Date', parse_dates=True, infer_datetime_format=True, dtype=object).sort_index()\n",
    "\n",
    "# # Merge dfs\n",
    "# spgsci_ER_df = spgsci1_df.combine_first(spgsci2_df)\n",
    "\n",
    "# # Push df to csv\n",
    "# spgsci_ER_df.to_csv(r'Data/spgsci_ER.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data\n",
    "# # Read path\n",
    "# es_path = Path('Data/ES=F.csv')\n",
    "# uso_path = Path('Data/USO.csv')\n",
    "\n",
    "# # Read files to a dataframe\n",
    "# es_df = pd.read_csv(es_path, index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "# uso_df = pd.read_csv(uso_path, index_col='Date', parse_dates=True, infer_datetime_format=True).sort_index()\n",
    "\n",
    "# # Add ticker symbol to Df\n",
    "# cl_df[\"Ticker\"] = \"CL\"\n",
    "# es_df[\"Ticker\"] = \"ES\"\n",
    "# uso_df[\"Ticker\"] = \"USO\"\n",
    "# uup_df[\"Ticker\"] = \"UUP\"\n",
    "\n",
    "# # Drop columns to have just closing price\n",
    "# es_close = es_df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n",
    "# uso_close = uso_df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n",
    "# spgsci_ER_close = spgsci_ER_df.drop(columns=['Open', 'High', 'Low'])\n",
    "# spgsci_TR_close = spgsci_TR_df.drop(columns=['Open', 'High', 'Low'])\n",
    "\n",
    "# # Change column name to ticker name\n",
    "# es_close.columns = ['ES']\n",
    "# uso_close.columns = ['USO']\n",
    "# spgsci_ER_close.columns = ['SPGSCI_ER']\n",
    "# spgsci_TR_close.columns = ['SPGSCI_TR']\n",
    "\n",
    "# # combine cl df with other dfs\n",
    "# combined_cl_es = pd.concat([cl_close, es_close], axis='columns', join='inner')\n",
    "# combined_cl_uso = pd.concat([cl_close,uso_close], axis='columns', join='inner')\n",
    "# combined_cl_spgsciER = pd.concat([cl_close,spgsci_ER_close], axis='columns', join='inner')\n",
    "# combined_cl_spgsciTR = pd.concat([cl_close,spgsci_TR_close], axis='columns', join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show plot with two Y's access\n",
    "# # Found these didn't provide any help to the project\n",
    "# show_plot_twoYs(combined_cl_spgsciER, combined_cl_spgsciER.index, 'CL', 'SPGSCI_ER')\n",
    "# show_plot_twoYs(combined_cl_spgsciTR, combined_cl_spgsciTR.index, 'CL', 'SPGSCI_TR')\n",
    "# show_plot_twoYs(combined_cl_uso, combined_cl_uso.index, 'CL', 'USO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
