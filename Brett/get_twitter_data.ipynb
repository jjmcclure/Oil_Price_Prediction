{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "from tweepy import OAuthHandler\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def get_twitter_data(screen_name):\n",
    "    # load data for auth to twitter\n",
    "    load_dotenv(dotenv_path=\"C:/Users/brett/Desktop/FTBC/.env\")\n",
    "    api_key = os.getenv('TWITTER_API_KEY')\n",
    "    api_secret_key = os.getenv('TWITTER_SECRET_KEY')\n",
    "    bearer_token = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "    access_token = os.getenv('TWITTER_ACCESS_TOKEN')\n",
    "    access_s_token = os.getenv(\"TWITTER_SECRET_TOKEN\")\n",
    "    \n",
    "    auth = tw.OAuthHandler(api_key, api_secret_key)\n",
    "    auth.set_access_token(access_token, access_s_token)\n",
    "\n",
    "    api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    # First request to get max count allowed to initialize user_tweets varible to allow iterating to get more tweets\n",
    "    tweets = []\n",
    "    total_tweets = 0\n",
    "    user_tweets = api.user_timeline(screen_name = screen_name,\n",
    "                                    include_rts=False,\n",
    "                                    tweet_mode='extended',\n",
    "                                    count = 200)\n",
    "    # add user_tweets to tweets list\n",
    "    tweets.extend(user_tweets)\n",
    "    total_tweets += len(user_tweets)\n",
    "\n",
    "    # loop to pull max amount of tweets allowed by twitter (3200)\n",
    "    while len(user_tweets) > 0:\n",
    "        # varialbe to be used as max_id for loop\n",
    "        last_id = user_tweets[-1].id - 1\n",
    "        \n",
    "        user_tweets = api.user_timeline(screen_name = screen_name,\n",
    "                                        max_id = last_id - 1,\n",
    "                                        include_rts=False,\n",
    "                                        tweet_mode='extended',\n",
    "                                        count = 200)\n",
    "        \n",
    "        # add additional tweets to original list\n",
    "        tweets.extend(user_tweets)\n",
    "        \n",
    "        print(f'{len(user_tweets)} have been downloaded for {screen_name}')\n",
    "        total_tweets += len(user_tweets) \n",
    "    \n",
    "    print(f'{total_tweets} downloaded for {screen_name}')\n",
    "    # extract the data that is needed from the tweet data\n",
    "    tweet_details = [[tweet.user.screen_name, tweet.created_at, tweet.full_text, tweet.favorite_count, tweet.retweet_count] for tweet in tweets]\n",
    "    \n",
    "    # Push extracted data to csv for use later\n",
    "    with open(f'Data/{screen_name}_tweets.csv', 'w', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['screen_name', 'date', 'tweet', 'likes', 'retweets'])\n",
    "        writer.writerows(tweet_details)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in json files\n",
    "boonepickens_path = Path('Data/boonepickens_tweets.csv')\n",
    "chrismartenson_path = Path('Data/chrismartenson_tweets.csv')\n",
    "chrisnelder_path = Path('Data/chrisnelder_tweets.csv')\n",
    "collineatonhc_path = Path('Data/CollinEatonHC_tweets.csv')\n",
    "gasbuddyguy_path = Path('Data/GasBuddyGuy_tweets.csv')\n",
    "jendlouhyhc_path = Path('Data/jendlouhyhc_tweets.csv')\n",
    "jkempenergy_path = Path('Data/JKempEnergy_tweets.csv')\n",
    "robinenergy_path = Path('Data/robinenergy_tweets.csv')\n",
    "staunovo_path = Path('Data/staunovo_tweets.csv')\n",
    "thearorareport_path = Path('Data/TheAroraReport_tweets.csv')\n",
    "\n",
    "# Convert to dfs\n",
    "boonepickens_df = pd.read_csv(boonepickens_path, parse_dates=True, infer_datetime_format=True)\n",
    "chrismartenson_df = pd.read_csv(chrismartenson_path, parse_dates=True, infer_datetime_format=True)\n",
    "chrisnelder_df = pd.read_csv(chrisnelder_path, parse_dates=True, infer_datetime_format=True)\n",
    "collineatonhc_df = pd.read_csv(collineatonhc_path, parse_dates=True, infer_datetime_format=True)\n",
    "gasbuddyguy_df = pd.read_csv(gasbuddyguy_path, parse_dates=True, infer_datetime_format=True)\n",
    "jendlouhyhc_df = pd.read_csv(jendlouhyhc_path, parse_dates=True, infer_datetime_format=True)\n",
    "jkempenergy_df = pd.read_csv(jkempenergy_path, parse_dates=True, infer_datetime_format=True)\n",
    "robinenergy_df = pd.read_csv(robinenergy_path, parse_dates=True, infer_datetime_format=True)\n",
    "staunovo_df = pd.read_csv(staunovo_path, parse_dates=True, infer_datetime_format=True)\n",
    "thearorareport_df = pd.read_csv(thearorareport_path, parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Combine into 1 dataframe\n",
    "all_tweets_df = pd.concat([boonepickens_df, chrismartenson_df,\n",
    "                         chrisnelder_df, collineatonhc_df,\n",
    "                         gasbuddyguy_df, jendlouhyhc_df,\n",
    "                         jkempenergy_df, robinenergy_df,\n",
    "                         staunovo_df, thearorareport_df]).reset_index(drop=True)\n",
    "\n",
    "# Normalize the Date field\n",
    "all_tweets_df['date'] = pd.to_datetime(all_tweets_df['date'], errors='coerce')\n",
    "all_tweets_df['date'] = all_tweets_df['date'].dt.normalize()\n",
    "\n",
    "# Remove the url from the tweet\n",
    "tweets_wo_url = []\n",
    "for tweet in all_tweets_df['tweet']:\n",
    "    no_url = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweets_wo_url.append({\n",
    "            \"tweets_no_url\": no_url}\n",
    "    )\n",
    "tweets_wo_url_df = pd.DataFrame(tweets_wo_url)\n",
    "\n",
    "# Append no_url_tweets to all_tweets_df and remove tweets column\n",
    "all_tweets_df= (pd.merge(all_tweets_df, tweets_wo_url_df, left_index=True, right_index=True, how='inner')\n",
    "                .drop(columns='tweet')\n",
    "               )\n",
    "\n",
    "# Display df\n",
    "pd.set_option('max_colwidth', 200)\n",
    "all_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets_sentiment = []\n",
    "\n",
    "for tweet in all_tweets_df['tweets_no_url']:\n",
    "    try:\n",
    "        text = tweet\n",
    "        sentiment = analyzer.polarity_scores(tweet)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "\n",
    "        tweets_sentiment.append({\n",
    "            \"vader compound\": compound,\n",
    "            \"vader positive\": pos,\n",
    "            \"vader negative\": neg,\n",
    "            \"vader neutral\": neu \n",
    "        })\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# create Df\n",
    "vader_df = pd.DataFrame(tweets_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textblob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Analysis\n",
    "blob_sentiment = []\n",
    "\n",
    "for tweet in all_tweets_df['tweets_no_url']:\n",
    "    blob = TextBlob(tweet)\n",
    "    blob.sentiment\n",
    "    polarity = blob.sentiment[0]\n",
    "    subjectivity = blob.sentiment[1]\n",
    "\n",
    "    blob_sentiment.append({\n",
    "        'Textblob Polarity': polarity,\n",
    "        'Textblob Subjectivity': subjectivity\n",
    "    })\n",
    "\n",
    "# create Df\n",
    "textblob_df = pd.DataFrame(blob_sentiment)\n",
    "textblob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all_tweets_df with the 2 sentiment dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_tweets_df with the 2 sentiment dataframes\n",
    "tweet_sentiment_df = pd.concat([all_tweets_df, vader_df, textblob_df], axis=\"columns\", join='inner')\n",
    "pd.set_option('max_colwidth', 100)\n",
    "tweet_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Df to csv file\n",
    "tweet_sentiment_df.to_csv('Data/tweet_sentiment_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
